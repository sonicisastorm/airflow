{"timestamp":"2025-06-15T12:33:07.451891","level":"info","event":"DAG bundles loaded: dags-folder","logger":"airflow.dag_processing.bundles.manager.DagBundlesManager"}
{"timestamp":"2025-06-15T12:33:07.453512","level":"info","event":"Filling up the DagBag from /opt/airflow/dags/spark_submit.py","logger":"airflow.models.dagbag.DagBag"}
{"timestamp":"2025-06-15T12:33:07.499818","level":"info","event":"Tmp dir root location: /tmp","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:07.500913","level":"info","event":"Running command: ['/usr/bin/bash', '-c', '\\n#!/bin/bash\\nspark-submit   --master spark://spark-master:7077   --deploy-mode client   --conf spark.eventLog.enabled=false   /opt/airflow/dags/hello_spark.py']","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:07.519713","level":"info","event":"Output:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:11.989759","level":"info","event":"25/06/15 12:33:11 INFO SparkContext: Running Spark version 3.5.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:11.994311","level":"info","event":"25/06/15 12:33:11 INFO SparkContext: OS info Linux, 6.8.0-55-generic, amd64","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:11.995339","level":"info","event":"25/06/15 12:33:11 INFO SparkContext: Java version 17.0.15","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.098114","level":"info","event":"25/06/15 12:33:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.265635","level":"info","event":"25/06/15 12:33:12 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.272267","level":"info","event":"25/06/15 12:33:12 INFO ResourceUtils: No custom resources configured for spark.driver.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.272610","level":"info","event":"25/06/15 12:33:12 INFO ResourceUtils: ==============================================================","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.272852","level":"info","event":"25/06/15 12:33:12 INFO SparkContext: Submitted application: HelloSpark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.279770","level":"info","event":"25/06/15 12:33:12 INFO SparkContext: Spark configuration:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.280125","level":"info","event":"spark.app.name=HelloSpark","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.280295","level":"info","event":"spark.app.startTime=1749990791971","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.280438","level":"info","event":"spark.app.submitTime=1749990790790","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.280591","level":"info","event":"spark.delta.logStore.class=org.apache.spark.sql.delta.storage.S3SingleDriverLogStore","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.280818","level":"info","event":"spark.driver.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.281088","level":"info","event":"spark.eventLog.dir=/opt/spark/history","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.281255","level":"info","event":"spark.eventLog.enabled=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.281415","level":"info","event":"spark.executor.extraJavaOptions=-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.281606","level":"info","event":"spark.hadoop.fs.s3a.access.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.281773","level":"info","event":"spark.hadoop.fs.s3a.endpoint=http://minio:9000","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285195","level":"info","event":"spark.hadoop.fs.s3a.fast.upload=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285414","level":"info","event":"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285504","level":"info","event":"spark.hadoop.fs.s3a.multiobjectdelete.enable=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285581","level":"info","event":"spark.hadoop.fs.s3a.path.style.access=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285653","level":"info","event":"spark.hadoop.fs.s3a.secret.key=*********(redacted)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285723","level":"info","event":"spark.history.fs.logDirectory=s3a://spark/","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285791","level":"info","event":"spark.logConf=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.285920","level":"info","event":"spark.master=spark://spark-master:7077","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286078","level":"info","event":"spark.network.timeout=10000s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286218","level":"info","event":"spark.rdd.compress=True","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286334","level":"info","event":"spark.serializer.objectStreamReset=100","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286447","level":"info","event":"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286562","level":"info","event":"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286676","level":"info","event":"spark.sql.files.ignoreMissingFiles=true","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286789","level":"info","event":"spark.sql.sources.partitionOverwriteMode=dynamic","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.286920","level":"info","event":"spark.submit.deployMode=client","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.287067","level":"info","event":"spark.submit.pyFiles=","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.322297","level":"info","event":"25/06/15 12:33:12 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.339273","level":"info","event":"25/06/15 12:33:12 INFO ResourceProfile: Limiting resource is cpu","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.339514","level":"info","event":"25/06/15 12:33:12 INFO ResourceProfileManager: Added ResourceProfile id: 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.428911","level":"info","event":"25/06/15 12:33:12 INFO SecurityManager: Changing view acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.429582","level":"info","event":"25/06/15 12:33:12 INFO SecurityManager: Changing modify acls to: airflow","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.430125","level":"info","event":"25/06/15 12:33:12 INFO SecurityManager: Changing view acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.430492","level":"info","event":"25/06/15 12:33:12 INFO SecurityManager: Changing modify acls groups to:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.436043","level":"info","event":"25/06/15 12:33:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: airflow; groups with view permissions: EMPTY; users with modify permissions: airflow; groups with modify permissions: EMPTY","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.876678","level":"info","event":"25/06/15 12:33:12 INFO Utils: Successfully started service 'sparkDriver' on port 45423.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:12.925883","level":"info","event":"25/06/15 12:33:12 INFO SparkEnv: Registering MapOutputTracker","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.000174","level":"info","event":"25/06/15 12:33:12 INFO SparkEnv: Registering BlockManagerMaster","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.040530","level":"info","event":"25/06/15 12:33:13 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.044553","level":"info","event":"25/06/15 12:33:13 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.053673","level":"info","event":"25/06/15 12:33:13 INFO SparkEnv: Registering BlockManagerMasterHeartbeat","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.117360","level":"info","event":"25/06/15 12:33:13 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-87c2ffd3-ac70-423c-bc9d-9359ed8ca9c2","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.142264","level":"info","event":"25/06/15 12:33:13 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.173554","level":"info","event":"25/06/15 12:33:13 INFO SparkEnv: Registering OutputCommitCoordinator","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.496916","level":"info","event":"25/06/15 12:33:13 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.616182","level":"info","event":"25/06/15 12:33:13 INFO Utils: Successfully started service 'SparkUI' on port 4040.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.890139","level":"info","event":"25/06/15 12:33:13 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:13.997969","level":"info","event":"25/06/15 12:33:13 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.8:7077 after 62 ms (0 ms spent in bootstraps)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.189381","level":"info","event":"25/06/15 12:33:14 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250615123314-0003","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.201881","level":"info","event":"25/06/15 12:33:14 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250615123314-0003/0 on worker-20250615115738-172.18.0.9-43281 (172.18.0.9:43281) with 1 core(s)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.209648","level":"info","event":"25/06/15 12:33:14 INFO StandaloneSchedulerBackend: Granted executor ID app-20250615123314-0003/0 on hostPort 172.18.0.9:43281 with 1 core(s), 1024.0 MiB RAM","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.225040","level":"info","event":"25/06/15 12:33:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36663.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.225369","level":"info","event":"25/06/15 12:33:14 INFO NettyBlockTransferService: Server created on 3e8077fa8995:36663","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.233378","level":"info","event":"25/06/15 12:33:14 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.259303","level":"info","event":"25/06/15 12:33:14 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3e8077fa8995, 36663, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.268293","level":"info","event":"25/06/15 12:33:14 INFO BlockManagerMasterEndpoint: Registering block manager 3e8077fa8995:36663 with 434.4 MiB RAM, BlockManagerId(driver, 3e8077fa8995, 36663, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.272956","level":"info","event":"25/06/15 12:33:14 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3e8077fa8995, 36663, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.275256","level":"info","event":"25/06/15 12:33:14 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3e8077fa8995, 36663, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.387472","level":"info","event":"25/06/15 12:33:14 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250615123314-0003/0 is now RUNNING","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:14.751105","level":"info","event":"25/06/15 12:33:14 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:15.678349","level":"info","event":"25/06/15 12:33:15 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:15.697274","level":"info","event":"25/06/15 12:33:15 INFO SharedState: Warehouse path is 'file:/tmp/airflowtmpybatmlaa/spark-warehouse'.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.094738","level":"info","event":"25/06/15 12:33:21 INFO CodeGenerator: Code generated in 385.773896 ms","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.174078","level":"info","event":"25/06/15 12:33:21 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.203087","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.204707","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.205406","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Parents of final stage: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.206651","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Missing parents: List()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.214258","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.358027","level":"info","event":"25/06/15 12:33:21 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 12.7 KiB, free 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.426165","level":"info","event":"25/06/15 12:33:21 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.6 KiB, free 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.430781","level":"info","event":"25/06/15 12:33:21 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3e8077fa8995:36663 (size: 6.6 KiB, free: 434.4 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.436730","level":"info","event":"25/06/15 12:33:21 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.456289","level":"info","event":"25/06/15 12:33:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:21.456705","level":"info","event":"25/06/15 12:33:21 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:23.295137","level":"info","event":"25/06/15 12:33:23 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:55080) with ID 0,  ResourceProfileId 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:23.597094","level":"info","event":"25/06/15 12:33:23 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:42761 with 413.9 MiB RAM, BlockManagerId(0, 172.18.0.9, 42761, None)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:23.756972","level":"info","event":"25/06/15 12:33:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 7636 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:24.517218","level":"info","event":"25/06/15 12:33:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:42761 (size: 6.6 KiB, free: 413.9 MiB)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932024","level":"info","event":"25/06/15 12:33:28 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.18.0.9 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932281","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932449","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932602","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932754","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.932965","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.933162","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.933393","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.933638","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.933884","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.934134","level":"info","event":"\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.934347","level":"info","event":"\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.934563","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.935181","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.935365","level":"info","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.935586","level":"info","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.935825","level":"info","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936050","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936237","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936416","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936574","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936729","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.936906","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937181","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937320","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937471","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937626","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937759","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937879","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.937980","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.938127","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.938308","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.938456","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.938630","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.938825","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:28.939086","level":"info","event":"25/06/15 12:33:28 INFO TaskSetManager: Starting task 0.1 in stage 0.0 (TID 1) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 7636 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.023398","level":"info","event":"25/06/15 12:33:29 INFO TaskSetManager: Lost task 0.1 in stage 0.0 (TID 1) on 172.18.0.9, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.023598","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.023718","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.023842","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.023980","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.024125","level":"info","event":") [duplicate 1]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.024239","level":"info","event":"25/06/15 12:33:29 INFO TaskSetManager: Starting task 0.2 in stage 0.0 (TID 2) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 7636 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.096416","level":"info","event":"25/06/15 12:33:29 INFO TaskSetManager: Lost task 0.2 in stage 0.0 (TID 2) on 172.18.0.9, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.096635","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.096752","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.096878","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.097024","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.097147","level":"info","event":") [duplicate 2]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.107840","level":"info","event":"25/06/15 12:33:29 INFO TaskSetManager: Starting task 0.3 in stage 0.0 (TID 3) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 7636 bytes)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.208418","level":"info","event":"25/06/15 12:33:29 INFO TaskSetManager: Lost task 0.3 in stage 0.0 (TID 3) on 172.18.0.9, executor 0: org.apache.spark.api.python.PythonException (Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.208697","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.208865","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.208998","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.209107","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.209207","level":"info","event":") [duplicate 3]","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.212154","level":"info","event":"25/06/15 12:33:29 ERROR TaskSetManager: Task 0 in stage 0.0 failed 4 times; aborting job","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.213481","level":"info","event":"25/06/15 12:33:29 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216052","level":"info","event":"25/06/15 12:33:29 INFO TaskSchedulerImpl: Cancelling stage 0","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216273","level":"info","event":"25/06/15 12:33:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.9 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216398","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216504","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216608","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216758","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.216911","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.217039","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.217142","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.218881","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219146","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219272","level":"info","event":"\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219377","level":"info","event":"\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219479","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219579","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219702","level":"info","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.219862","level":"info","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220022","level":"info","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220192","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220328","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220458","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220588","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220725","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.220874","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.222853","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223030","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223167","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223296","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223421","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223526","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223628","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223730","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223851","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.223972","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224073","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224174","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224277","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224391","level":"info","event":"25/06/15 12:33:29 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) failed in 7.973 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.9 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224539","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224646","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224748","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.224869","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228050","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228275","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228425","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228539","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228669","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.228832","level":"info","event":"\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229009","level":"info","event":"\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229146","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229263","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229368","level":"info","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229474","level":"info","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229580","level":"info","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229704","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.229865","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230186","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230356","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230488","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230589","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230687","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230785","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.230901","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231017","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231127","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231242","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231340","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231437","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231535","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231659","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231763","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.231912","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.232057","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.232169","level":"info","event":"25/06/15 12:33:29 INFO DAGScheduler: Job 0 failed: showString at NativeMethodAccessorImpl.java:0, took 8.053906 s","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.568374","level":"info","event":"Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.568657","level":"info","event":"  File \"/opt/airflow/dags/hello_spark.py\", line 5, in <module>","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.568853","level":"info","event":"    spark.createDataFrame([(\"Alice\", 25), (\"Bob\", 33)], [\"name\", \"age\"]).show()","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.569007","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 959, in show","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.569132","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.569251","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.569373","level":"info","event":"  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.610907","level":"info","event":"py4j.protocol.Py4JJavaError: An error occurred while calling o58.showString.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.611228","level":"info","event":": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3) (172.18.0.9 executor 0): org.apache.spark.api.python.PythonException: Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.611402","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.611553","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.611692","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.611857","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612031","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612168","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612322","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612468","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612589","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612733","level":"info","event":"\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.612895","level":"info","event":"\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613054","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613199","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613343","level":"info","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613481","level":"info","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613624","level":"info","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613777","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.613990","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614149","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614293","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614428","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614573","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614722","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.614876","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615062","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615211","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615350","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615494","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615641","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615776","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.615960","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616113","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616246","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616401","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616547","level":"info","event":"Driver stacktrace:","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616689","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.616850","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617044","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617189","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617334","level":"info","event":"\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617486","level":"info","event":"\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617625","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617776","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.617961","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618111","level":"info","event":"\tat scala.Option.foreach(Option.scala:407)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618264","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618435","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618572","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618726","level":"info","event":"\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.618898","level":"info","event":"\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619068","level":"info","event":"\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619225","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619371","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619510","level":"info","event":"\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619646","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619789","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.619980","level":"info","event":"\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620136","level":"info","event":"\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620294","level":"info","event":"\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620441","level":"info","event":"\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620582","level":"info","event":"\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620720","level":"info","event":"\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.620862","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621039","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621183","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621313","level":"info","event":"\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621448","level":"info","event":"\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621587","level":"info","event":"\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621717","level":"info","event":"\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.621869","level":"info","event":"\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622032","level":"info","event":"\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622172","level":"info","event":"\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622307","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622451","level":"info","event":"\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622589","level":"info","event":"\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622721","level":"info","event":"\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.622881","level":"info","event":"\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623048","level":"info","event":"\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623184","level":"info","event":"\tat py4j.Gateway.invoke(Gateway.java:282)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623329","level":"info","event":"\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623468","level":"info","event":"\tat py4j.commands.CallCommand.execute(CallCommand.java:79)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623609","level":"info","event":"\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623755","level":"info","event":"\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.623922","level":"info","event":"\tat java.base/java.lang.Thread.run(Thread.java:840)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624081","level":"info","event":"Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624227","level":"info","event":"  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1100, in main","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624367","level":"info","event":"    raise PySparkRuntimeError(","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624508","level":"info","event":"pyspark.errors.exceptions.base.PySparkRuntimeError: [PYTHON_VERSION_MISMATCH] Python in worker has different version (3, 10) than that in driver 3.11, PySpark cannot run with different minor versions.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624661","level":"info","event":"Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624819","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.624987","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625144","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625285","level":"info","event":"\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625417","level":"info","event":"\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625562","level":"info","event":"\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625708","level":"info","event":"\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.625855","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626021","level":"info","event":"\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626165","level":"info","event":"\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626320","level":"info","event":"\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626476","level":"info","event":"\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626621","level":"info","event":"\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626753","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.626920","level":"info","event":"\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627083","level":"info","event":"\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627201","level":"info","event":"\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627345","level":"info","event":"\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627490","level":"info","event":"\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627624","level":"info","event":"\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627765","level":"info","event":"\tat org.apache.spark.scheduler.Task.run(Task.scala:141)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.627950","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628092","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628240","level":"info","event":"\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628386","level":"info","event":"\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628523","level":"info","event":"\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628661","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628823","level":"info","event":"\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.628983","level":"info","event":"\t... 1 more","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.629141","level":"info","event":"","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.673050","level":"info","event":"25/06/15 12:33:29 INFO SparkContext: Invoking stop() from shutdown hook","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.673533","level":"info","event":"25/06/15 12:33:29 INFO SparkContext: SparkContext is stopping with exitCode 0.","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.693860","level":"info","event":"25/06/15 12:33:29 INFO SparkUI: Stopped Spark web UI at http://3e8077fa8995:4040","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.702079","level":"info","event":"25/06/15 12:33:29 INFO StandaloneSchedulerBackend: Shutting down all executors","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.702887","level":"info","event":"25/06/15 12:33:29 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.731246","level":"info","event":"25/06/15 12:33:29 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.752965","level":"info","event":"25/06/15 12:33:29 INFO MemoryStore: MemoryStore cleared","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.753243","level":"info","event":"25/06/15 12:33:29 INFO BlockManager: BlockManager stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.767302","level":"info","event":"25/06/15 12:33:29 INFO BlockManagerMaster: BlockManagerMaster stopped","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.770547","level":"info","event":"25/06/15 12:33:29 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.782865","level":"info","event":"25/06/15 12:33:29 INFO SparkContext: Successfully stopped SparkContext","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.783144","level":"info","event":"25/06/15 12:33:29 INFO ShutdownHookManager: Shutdown hook called","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.786124","level":"info","event":"25/06/15 12:33:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c09535e-23c0-48bc-a15d-907d4598b4c8/pyspark-894aa8de-3640-445d-8096-dde7ad958895","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.789651","level":"info","event":"25/06/15 12:33:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-6c09535e-23c0-48bc-a15d-907d4598b4c8","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.796090","level":"info","event":"25/06/15 12:33:29 INFO ShutdownHookManager: Deleting directory /tmp/spark-95e2536c-5e99-4466-ae52-bff0a0f1a5c7","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.878668","level":"info","event":"Command exited with return code 1","logger":"airflow.task.hooks.airflow.providers.standard.hooks.subprocess.SubprocessHook"}
{"timestamp":"2025-06-15T12:33:29.879992","level":"error","event":"Task failed with exception","logger":"task","error_detail":[{"exc_type":"AirflowException","exc_value":"Bash command failed. The command returned a non-zero exit code 1.","exc_notes":[],"syntax_error":null,"is_cause":false,"frames":[{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":838,"name":"run"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/execution_time/task_runner.py","lineno":1130,"name":"_execute_task"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/sdk/bases/operator.py","lineno":408,"name":"wrapper"},{"filename":"/home/airflow/.local/lib/python3.10/site-packages/airflow/providers/standard/operators/bash.py","lineno":233,"name":"execute"}]}]}
